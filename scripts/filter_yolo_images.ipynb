{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christian/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/christian/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <B3E58761-2785-34C6-A89B-F37110C88A05> /Users/christian/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <CA0A91CD-08B1-3B88-A2D5-BD93563ECA22> /Users/christian/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/christian/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "from utils.dir_utils import mkdir, get_last_path\n",
    "from utils.model_utils import load_checkpoint\n",
    "from model.URSCT_model import URSCT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 43586)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def get_all_images_in_directory(directory):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if is_image_file(file):\n",
    "                images.append(os.path.join(root, file))\n",
    "    return images\n",
    "\n",
    "habitat_frame_regex = re.compile(r'(\\d{4})_(.+)_(f\\d+)[_.](?=jpg|png)')\n",
    "\n",
    "# add all images in the directory to the list\n",
    "\n",
    "original_dataset_path = \"../../eda/data/DeepFish/\"\n",
    "\n",
    "yolo_images = get_all_images_in_directory(\"../../yolo/datasets/DeepFish-2/test\")\n",
    "original_images = (\n",
    "    get_all_images_in_directory(original_dataset_path + \"Classification\")\n",
    "    + get_all_images_in_directory(original_dataset_path + \"Localization/images\")\n",
    "    + get_all_images_in_directory(original_dataset_path + \"Segmentation/images\")\n",
    ")\n",
    "\n",
    "\n",
    "len(yolo_images), len(original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../yolo/datasets/DeepFish-2/test/images/7117_Caranx_sexfasciatus_juvenile_f001410_jpg.rf.2f7f30b91471cf33eceabed996e36cc6.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/9866_no_fish_f000009_jpg.rf.1b691c7ed3043cedc5f1eb3e4f4e7b21.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7398_NF2_f000181_jpg.rf.06beabe2773409a1ba4b5305832a3051.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7268_F1_f000301_jpg.rf.de9e00afda3d3f8a673439ae8fedde39.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7434_NF2_f000061_jpg.rf.c3ff9344882f61c38483d01fa08ec2ff.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39766, 43586)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_images_dict = {\n",
    "    re.match(habitat_frame_regex, image.split(\"/\")[-1]).groups(0): image\n",
    "    for image in original_images\n",
    "}\n",
    "# lowercase keys in the dictionary\n",
    "original_images_dict = {\n",
    "    tuple(\n",
    "        key_part.lower() if isinstance(key_part, str) else key_part for key_part in key\n",
    "    ): value\n",
    "    for key, value in original_images_dict.items()\n",
    "}\n",
    "\n",
    "\n",
    "len(original_images_dict), len(\n",
    "    original_images\n",
    ")  # duplicate keys are removed, i hope this is not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('9892', 'acanthopagrus_palmaris', 'f000038'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_acanthopagrus_palmaris_f000038.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris', 'f000010'),\n",
       "  '../../eda/data/DeepFish/Segmentation/images/valid/9892_acanthopagrus_palmaris_f000010.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris', 'f000004'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_acanthopagrus_palmaris_f000004.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris_2', 'f000008'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_Acanthopagrus_palmaris_2_f000008.jpg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(original_images_dict.items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading successfully!\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/Enh_opt.yaml', 'r') as config:\n",
    "    opt = yaml.safe_load(config)\n",
    "    opt_test = opt['DEEPFISH']\n",
    "\n",
    "device = opt_test['DEVICE']\n",
    "model_detail_opt = opt['MODEL_DETAIL']\n",
    "result_dir = os.path.join(opt_test['SAVE_DIR'], opt['TRAINING']['MODEL_NAME'], 'test_results')\n",
    "mkdir(result_dir)\n",
    "\n",
    "model = URSCT(model_detail_opt).to(device)\n",
    "path_chk_rest = get_last_path(os.path.join(opt_test['SAVE_DIR'], opt['TRAINING']['MODEL_NAME'], 'models'), '_bestSSIM.pth')\n",
    "load_checkpoint(model, path_chk_rest, device)\n",
    "model.eval()\n",
    "\n",
    "patch_size = opt_test['TEST_PS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_letterbox(image, target_size=(640, 640)):\n",
    "    orig_w, orig_h = image.size\n",
    "    \n",
    "    scale = min(target_size[0] / orig_h, target_size[1] / orig_w)\n",
    "    \n",
    "    new_w = int(orig_w * scale)\n",
    "    new_h = int(orig_h * scale)\n",
    "    \n",
    "    resized_img = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    \n",
    "    letterbox_img = Image.new('RGB', target_size, (0, 0, 0))\n",
    "    \n",
    "    top_left_x = (target_size[1] - new_w) // 2\n",
    "    top_left_y = (target_size[0] - new_h) // 2\n",
    "    \n",
    "    letterbox_img.paste(resized_img, (top_left_x, top_left_y))\n",
    "    \n",
    "    return letterbox_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [08:07<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# goal:\n",
    "# for each image with yolo annotation, get original image\n",
    "# for each filtered image, run swin transformer\n",
    "# write the output to a new directory with division test train val\n",
    "# transform yolo annotations to the new image size (multiply y axis by 16/9)\n",
    "\n",
    "for yolo_image in tqdm(yolo_images):\n",
    "    habitat, fish_type, frame = re.match(\n",
    "        habitat_frame_regex, yolo_image.split(\"/\")[-1]\n",
    "    ).groups(0)\n",
    "    original_image_path = original_images_dict.get((habitat, fish_type.lower(), frame))\n",
    "\n",
    "    if original_image_path is None:\n",
    "        raise ValueError(\n",
    "            f\"Original image not found for {yolo_image} with key {(habitat, fish_type.lower(), frame)}\"\n",
    "        )\n",
    "\n",
    "    yolo_image_path = Path(yolo_image)\n",
    "    model_stage = str(yolo_image_path.parent.parent.name)\n",
    "    target_path = os.path.join(result_dir, model_stage, \"images\", yolo_image_path.stem + \".png\")\n",
    "\n",
    "    if os.path.exists(target_path):\n",
    "        continue\n",
    "\n",
    "    # generate swin image\n",
    "    inp_img = Image.open(original_image_path)\n",
    "\n",
    "    orig_w, orig_h = inp_img.size\n",
    "\n",
    "    # letterboxed_img = resize_with_letterbox(inp_img, (640, 640))\n",
    "    inp_img = TF.to_tensor(inp_img)\n",
    "    inp_img = TF.resize(inp_img, (patch_size[0], patch_size[1]))\n",
    "\n",
    "    image_gpu = inp_img.to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        restored_SR = model(image_gpu)\n",
    "\n",
    "    restored_SR = TF.resize(restored_SR, (int(patch_size[1] / (orig_w / orig_h)), patch_size[0]), interpolation=TF.InterpolationMode.BILINEAR)\n",
    "    \n",
    "    # save swin image to output directory\n",
    "    # create directories if they don't exist\n",
    "    mkdir(os.path.join(result_dir, model_stage, \"images\"))\n",
    "\n",
    "    torchvision.utils.save_image(\n",
    "        restored_SR[0],\n",
    "        # image_gpu[0],\n",
    "        target_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video_0015.mp4\n",
      "Processing 9894.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 157/157 [01:52<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7463.mp4\n",
      "Processing 9908.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 91/91 [01:04<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .DS_Store\n",
      "Processing 9892.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 55/55 [00:39<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7117.mp4\n",
      "Processing 9852.mp4\n",
      "Processing 7398.mp4\n",
      "Processing 7434.mp4\n",
      "Processing 7623.mp4\n",
      "Processing .mp4\n",
      "Processing 7393.mp4\n",
      "Processing 7426.mp4\n",
      "Processing Video_0051.mp4\n",
      "Processing 7585.mp4\n",
      "Processing Video_0034.mp4\n",
      "Processing 9862.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 45/45 [00:31<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7482.mp4\n",
      "Processing 9870.mp4\n",
      "Processing 9866.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 113/113 [01:21<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9907.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 186/186 [02:13<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9898.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 57/57 [00:40<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video_0030.mp4\n",
      "Processing 7490.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for input_video in os.listdir(\"../../eda/videos\"):\n",
    "    output_video_path = f\"swin/{input_video}\"\n",
    "    input_video_path = f\"../../eda/videos/{input_video}\"\n",
    "\n",
    "    mkdir(os.path.dirname(output_video_path))\n",
    "\n",
    "    if not input_video_path.endswith(\".mp4\"):\n",
    "        continue\n",
    "\n",
    "    if not input_video.split(\".\")[0] in [\n",
    "        \"9908\",\n",
    "        \"9907\",\n",
    "        \"9898\",\n",
    "        \"9894\",\n",
    "        \"9892\",\n",
    "        \"9862\",\n",
    "        \"9866\",\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {input_video}\")\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_dim = (360, 640)\n",
    "    out = cv2.VideoWriter(\n",
    "        output_video_path, fourcc, fps, (output_dim[1], output_dim[0])\n",
    "    )\n",
    "\n",
    "    for frame_number in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        orig_h, orig_w, _ = frame.shape\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        inp_img = TF.to_tensor(frame_rgb)\n",
    "        inp_img = TF.resize(inp_img, (patch_size[0], patch_size[1]))\n",
    "\n",
    "        image_gpu = inp_img.to(device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            restored_SR = model(image_gpu)\n",
    "\n",
    "        restored_SR = TF.resize(\n",
    "            restored_SR, output_dim, interpolation=TF.InterpolationMode.BILINEAR\n",
    "        )\n",
    "\n",
    "        output_frame = restored_SR.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        output_image = np.clip(output_frame * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # plot image\n",
    "        # plt.imshow(output_frame)\n",
    "        # plt.show()\n",
    "\n",
    "        out.write(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
