{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "from utils.dir_utils import mkdir, get_last_path\n",
    "from utils.model_utils import load_checkpoint\n",
    "from model.URSCT_model import URSCT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 43586)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def get_all_images_in_directory(directory):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if is_image_file(file):\n",
    "                images.append(os.path.join(root, file))\n",
    "    return images\n",
    "\n",
    "habitat_frame_regex = re.compile(r'(\\d{4})_(.+)_(f\\d+)[_.](?=jpg|png)')\n",
    "\n",
    "# add all images in the directory to the list\n",
    "\n",
    "original_dataset_path = \"../../eda/data/DeepFish/\"\n",
    "\n",
    "yolo_images = get_all_images_in_directory(\"../../yolo/datasets/DeepFish-2/test\")\n",
    "original_images = (\n",
    "    get_all_images_in_directory(original_dataset_path + \"Classification\")\n",
    "    + get_all_images_in_directory(original_dataset_path + \"Localization/images\")\n",
    "    + get_all_images_in_directory(original_dataset_path + \"Segmentation/images\")\n",
    ")\n",
    "\n",
    "\n",
    "len(yolo_images), len(original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../yolo/datasets/DeepFish-2/test/images/7117_Caranx_sexfasciatus_juvenile_f001410_jpg.rf.2f7f30b91471cf33eceabed996e36cc6.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/9866_no_fish_f000009_jpg.rf.1b691c7ed3043cedc5f1eb3e4f4e7b21.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7398_NF2_f000181_jpg.rf.06beabe2773409a1ba4b5305832a3051.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7268_F1_f000301_jpg.rf.de9e00afda3d3f8a673439ae8fedde39.jpg',\n",
       " '../../yolo/datasets/DeepFish-2/test/images/7434_NF2_f000061_jpg.rf.c3ff9344882f61c38483d01fa08ec2ff.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39766, 43586)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_images_dict = {\n",
    "    re.match(habitat_frame_regex, image.split(\"/\")[-1]).groups(0): image\n",
    "    for image in original_images\n",
    "}\n",
    "# lowercase keys in the dictionary\n",
    "original_images_dict = {\n",
    "    tuple(\n",
    "        key_part.lower() if isinstance(key_part, str) else key_part for key_part in key\n",
    "    ): value\n",
    "    for key, value in original_images_dict.items()\n",
    "}\n",
    "\n",
    "\n",
    "len(original_images_dict), len(\n",
    "    original_images\n",
    ")  # duplicate keys are removed, i hope this is not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('9892', 'acanthopagrus_palmaris', 'f000038'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_acanthopagrus_palmaris_f000038.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris', 'f000010'),\n",
       "  '../../eda/data/DeepFish/Segmentation/images/valid/9892_acanthopagrus_palmaris_f000010.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris', 'f000004'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_acanthopagrus_palmaris_f000004.jpg'),\n",
       " (('9892', 'acanthopagrus_palmaris_2', 'f000008'),\n",
       "  '../../eda/data/DeepFish/Classification/9892/valid/9892_Acanthopagrus_palmaris_2_f000008.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(original_images_dict.items())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading successfully!\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/Enh_opt.yaml', 'r') as config:\n",
    "    opt = yaml.safe_load(config)\n",
    "    opt_test = opt['DEEPFISH']\n",
    "\n",
    "device = opt_test['DEVICE']\n",
    "model_detail_opt = opt['MODEL_DETAIL']\n",
    "result_dir = os.path.join(opt_test['SAVE_DIR'], opt['TRAINING']['MODEL_NAME'], 'test_results')\n",
    "mkdir(result_dir)\n",
    "\n",
    "model = URSCT(model_detail_opt).to(device)\n",
    "path_chk_rest = get_last_path(os.path.join(opt_test['SAVE_DIR'], opt['TRAINING']['MODEL_NAME'], 'models'), '_bestSSIM.pth')\n",
    "load_checkpoint(model, path_chk_rest, device)\n",
    "model.eval()\n",
    "\n",
    "patch_size = opt_test['TEST_PS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_letterbox(image, target_size=(640, 640)):\n",
    "    orig_w, orig_h = image.size\n",
    "    \n",
    "    scale = min(target_size[0] / orig_h, target_size[1] / orig_w)\n",
    "    \n",
    "    new_w = int(orig_w * scale)\n",
    "    new_h = int(orig_h * scale)\n",
    "    \n",
    "    resized_img = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    \n",
    "    letterbox_img = Image.new('RGB', target_size, (0, 0, 0))\n",
    "    \n",
    "    top_left_x = (target_size[1] - new_w) // 2\n",
    "    top_left_y = (target_size[0] - new_h) // 2\n",
    "    \n",
    "    letterbox_img.paste(resized_img, (top_left_x, top_left_y))\n",
    "    \n",
    "    return letterbox_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [00:00<00:00, 19510.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# goal:\n",
    "# for each image with yolo annotation, get original image\n",
    "# for each filtered image, run swin transformer\n",
    "# write the output to a new directory with division test train val\n",
    "# transform yolo annotations to the new image size (multiply y axis by 16/9)\n",
    "\n",
    "for yolo_image in tqdm(yolo_images):\n",
    "    habitat, fish_type, frame = re.match(\n",
    "        habitat_frame_regex, yolo_image.split(\"/\")[-1]\n",
    "    ).groups(0)\n",
    "    original_image_path = original_images_dict.get((habitat, fish_type.lower(), frame))\n",
    "\n",
    "    if original_image_path is None:\n",
    "        raise ValueError(\n",
    "            f\"Original image not found for {yolo_image} with key {(habitat, fish_type.lower(), frame)}\"\n",
    "        )\n",
    "\n",
    "    yolo_image_path = Path(yolo_image)\n",
    "    model_stage = str(yolo_image_path.parent.parent.name)\n",
    "    target_path = os.path.join(result_dir, model_stage, \"images\", yolo_image_path.stem + \".png\")\n",
    "\n",
    "    if os.path.exists(target_path):\n",
    "        continue\n",
    "\n",
    "    # generate swin image\n",
    "    inp_img = Image.open(original_image_path)\n",
    "\n",
    "    orig_w, orig_h = inp_img.size\n",
    "\n",
    "    # letterboxed_img = resize_with_letterbox(inp_img, (640, 640))\n",
    "    inp_img = TF.to_tensor(inp_img)\n",
    "    inp_img = TF.resize(inp_img, (patch_size[0], patch_size[1]))\n",
    "\n",
    "    image_gpu = inp_img.to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        restored_SR = model(image_gpu)\n",
    "\n",
    "    restored_SR = TF.resize(restored_SR.cpu(), (int(patch_size[1] / (orig_w / orig_h)), patch_size[0]), interpolation=TF.InterpolationMode.BILINEAR)\n",
    "    \n",
    "    # save swin image to output directory\n",
    "    # create directories if they don't exist\n",
    "    mkdir(os.path.join(result_dir, model_stage, \"images\"))\n",
    "\n",
    "    torchvision.utils.save_image(\n",
    "        restored_SR[0],\n",
    "        # image_gpu[0],\n",
    "        target_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9894.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/157 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     51\u001b[0m     restored_SR \u001b[38;5;241m=\u001b[39m model(image_gpu)\n\u001b[0;32m---> 53\u001b[0m restored_SR \u001b[38;5;241m=\u001b[39m \u001b[43mTF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestored_SR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterpolationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBILINEAR\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m output_frame \u001b[38;5;241m=\u001b[39m restored_SR\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     58\u001b[0m output_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(output_frame \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/JBG060/URSCT-SESR/venv/lib/python3.9/site-packages/torch/nn/functional.py:4565\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4563\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[0;32m-> 4565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4566\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for input_video in os.listdir(\"../../eda/videos\"):\n",
    "    output_video_path = f\"swin/{input_video}\"\n",
    "    input_video_path = f\"../../eda/videos/{input_video}\"\n",
    "\n",
    "    mkdir(os.path.dirname(output_video_path))\n",
    "\n",
    "    if not input_video_path.endswith(\".mp4\"):\n",
    "        continue\n",
    "\n",
    "    if not input_video.split(\".\")[0] in [\n",
    "        \"9908\",\n",
    "        \"9907\",\n",
    "        \"9898\",\n",
    "        \"9894\",\n",
    "        \"9892\",\n",
    "        \"9862\",\n",
    "        \"9866\",\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {input_video}\")\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_dim = (360, 640)\n",
    "    out = cv2.VideoWriter(\n",
    "        output_video_path, fourcc, fps, (output_dim[1], output_dim[0])\n",
    "    )\n",
    "\n",
    "    for frame_number in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        orig_h, orig_w, _ = frame.shape\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        inp_img = TF.to_tensor(frame_rgb)\n",
    "        inp_img = TF.resize(inp_img, (patch_size[0], patch_size[1]))\n",
    "\n",
    "        image_gpu = inp_img.to(device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            restored_SR = model(image_gpu)\n",
    "\n",
    "        restored_SR = TF.resize(\n",
    "            restored_SR.cpu(), output_dim, interpolation=TF.InterpolationMode.BILINEAR\n",
    "        )\n",
    "\n",
    "        output_frame = restored_SR.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        output_image = np.clip(output_frame * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # plot image\n",
    "        # plt.imshow(output_frame)\n",
    "        # plt.show()\n",
    "\n",
    "        out.write(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
